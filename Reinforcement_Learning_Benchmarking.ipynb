{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.26.2'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "gym.__version__\n",
    "# 0.23.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, buffer_size, alpha=0.6, beta=0.4):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        self.priorities = np.zeros((buffer_size,), dtype=np.float32)\n",
    "\n",
    "    def add(self, state, action, reward, state_, done):\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "\n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append((state, action, reward, state_, done))\n",
    "        else:\n",
    "            self.buffer[self.position] = (state, action, reward, state_, done)\n",
    "        \n",
    "        self.priorities[self.position] = max_prio\n",
    "        self.position = (self.position + 1) % self.buffer_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        if len(self.buffer) == self.buffer_size:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.position]\n",
    "        \n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-self.beta)\n",
    "        weights /= weights.max()\n",
    "        weights = np.array(weights, dtype=np.float32)\n",
    "        \n",
    "        batch = list(zip(*samples))\n",
    "        states = torch.tensor(np.array(batch[0]), dtype=torch.float32)\n",
    "        actions = torch.tensor(np.array(batch[1]), dtype=torch.int64)\n",
    "        rewards = torch.tensor(np.array(batch[2]), dtype=torch.float32)\n",
    "        states_ = torch.tensor(np.array(batch[3]), dtype=torch.float32)\n",
    "        dones = torch.tensor(np.array(batch[4]), dtype=torch.int64)\n",
    "        return states, actions, rewards, states_, dones, indices, weights\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio\n",
    "    def betas_increment(self,betas):\n",
    "        self.betas=betas\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network and Agent Classes\n",
    "\n",
    "# Define a neural network class that inherits from the PyTorch nn.Module class.\n",
    "class LinearDeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, n_actions, input_dims,arc):\n",
    "        super(LinearDeepQNetwork, self).__init__()\n",
    "        self.arc=arc\n",
    "        if arc==\"Double\" or arc==\"Single\":\n",
    "            # Define the neural network layers and activation functions.\n",
    "            self.fc1 = nn.Linear(input_dims[0], 8)\n",
    "            self.fc2 = nn.Linear(8, 16)\n",
    "            self.fc3 = nn.Linear(16, 8)\n",
    "            self.fc4 = nn.Linear(8, n_actions)\n",
    "        elif arc==\"Dueling\" or arc==\"Double Dueling\":\n",
    "            self.fc1 = nn.Linear(input_dims[0], 8)\n",
    "            self.fc2 = nn.Linear(8, 16)\n",
    "            self.fc3 = nn.Linear(16, 8)\n",
    "            self.fc4 = nn.Linear(8, n_actions)\n",
    "\n",
    "            self.Values1 = nn.Linear(n_actions, 8)\n",
    "            self.Values2 = nn.Linear(8, 16)\n",
    "            self.Values3 = nn.Linear(16, 1)\n",
    "\n",
    "            self.advantage1 = nn.Linear(n_actions, 8)\n",
    "            self.advantage2 = nn.Linear(8, 16)\n",
    "            self.advantage3 = nn.Linear(16, n_actions)\n",
    "\n",
    "\n",
    "        # Define the optimizer and loss function for training the neural network.\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.HuberLoss()\n",
    "        self.device = torch.device('cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    # Define the forward pass of the neural network.\n",
    "    def forward(self, state):\n",
    "        if self.arc==\"Dueling\" or self.arc==\"Double Dueling\":\n",
    "            z = torch.relu(self.fc1(state))\n",
    "            z = torch.relu(self.fc2(z))\n",
    "            z = torch.relu(self.fc3(z))\n",
    "            z = self.fc4(z)\n",
    "\n",
    "            values=torch.relu(self.Values1(z))\n",
    "            values=torch.relu(self.Values2(values))\n",
    "            values=(self.Values3(values))\n",
    "\n",
    "            advantage=torch.relu(self.advantage1(z))\n",
    "            advantage=torch.relu(self.advantage2(advantage))\n",
    "            advantage=(self.advantage3(advantage))\n",
    "\n",
    "            advantage_means=torch.mean(advantage)\n",
    "            qvals = values + (advantage - advantage_means.view(-1,1))\n",
    "\n",
    "            return qvals\n",
    "        elif self.arc==\"Double\" or self.arc==\"Single\":\n",
    "            layer1 = torch.relu(self.fc1(state))\n",
    "            layer2 = torch.relu(self.fc2(layer1))\n",
    "            layer3 = torch.relu(self.fc3(layer2))\n",
    "            qvals = self.fc4(layer3)\n",
    "\n",
    "            return qvals\n",
    "\n",
    "\n",
    "# Define an agent class for training the neural network.\n",
    "class Agent():\n",
    "    def __init__(self, input_dims, n_actions,arc, buffer_size=2500, lr=5e-4, gamma=0.99,\n",
    "                epsilon=1.0, eps_min=0.01):\n",
    "        self.betas=0.4\n",
    "        self.lr = lr\n",
    "        self.input_dims = input_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_min\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "        self.arc=arc\n",
    "        self.buffer_size=buffer_size\n",
    "\n",
    "        # Create instances of the neural networks for the agent.\n",
    "        if self.arc==\"Single\" or self.arc==\"Dueling\":\n",
    "            self.Q_network = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims,self.arc)\n",
    "        else:\n",
    "            self.Q_network = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims,self.arc)\n",
    "            self.Target_network = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims,self.arc)\n",
    "            # Create Priority Replay Buffer\n",
    "        self.priority_replay_buffer = PrioritizedReplayBuffer(buffer_size,self.betas)\n",
    "\n",
    "    # Update the target network initially to match the online network.\n",
    "    def update_target_network(self):\n",
    "        if self.arc==\"Single\" or self.arc==\"Dueling\":\n",
    "            pass\n",
    "        else:\n",
    "            self.Target_network.load_state_dict(self.Q_network.state_dict())\n",
    "\n",
    "    # Define a function for choosing an action given an observation.\n",
    "    def choose_action(self, state):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            # Use the neural network to predict the Q-values for the current state.\n",
    "            state = torch.tensor(state, dtype=torch.float).to(self.Q_network.device)\n",
    "            actions = self.Q_network.forward(state)\n",
    "            # Choose the action with the highest Q-value.\n",
    "            action = torch.argmax(actions).item()\n",
    "        else:\n",
    "            # Choose a random action with probability epsilon.\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "\n",
    "    # Define a function for decrementing epsilon over time to decrease exploration.\n",
    "    def decrement_epsilon(self,decrement):\n",
    "        self.epsilon = self.epsilon - decrement if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    # Define a function for training the neural network with a batch of experiences.\n",
    "    def learn(self, states, actions, rewards, states_, dones,indices, weights):\n",
    "        self.Q_network.optimizer.zero_grad()\n",
    "\n",
    "        # Convert the data to PyTorch tensors and move to the device for training.\n",
    "        states = states.to(self.Q_network.device)\n",
    "        actions = actions.to(self.Q_network.device)\n",
    "        rewards = rewards.to(self.Q_network.device)\n",
    "        weights = torch.tensor(weights).to(self.Q_network.device)\n",
    "        if self.arc==\"Dueling\" or self.arc==\"Single\":\n",
    "            states_ = states_.to(self.Q_network.device)\n",
    "            dones = dones.to(self.Q_network.device)\n",
    "\n",
    "            # Use the online network to predict the Q-values for the current states and select actions.\n",
    "            q_pred = self.Q_network.forward(states).gather(1, actions.reshape(-1, 1))\n",
    "\n",
    "            # Use the target network to predict the Q-values for the next states.\n",
    "            q_next, _ = torch.max(self.Q_network.forward(states_), dim=1)\n",
    "            \n",
    "        else:\n",
    "            states_ = states_.to(self.Target_network.device)\n",
    "            dones = dones.to(self.Target_network.device)\n",
    "\n",
    "            # Use the online network to predict the Q-values for the current states and select actions.\n",
    "            q_pred = self.Q_network.forward(states).gather(1, actions.reshape(-1, 1))\n",
    "            \n",
    "            # Use the target network to predict the Q-values for the next states.\n",
    "            q_next, _ = torch.max(self.Target_network.forward(states_), dim=1)\n",
    "\n",
    "        # Calculate the target Q-values based on the current rewards and expected future rewards.\n",
    "        q_target = rewards + self.gamma * (1-dones) * q_next\n",
    "        q_target = q_target.detach()\n",
    "\n",
    "        # Calculate the mean squared error loss between the predicted and target Q-values.\n",
    "        loss  = (q_pred.squeeze() - q_target).pow(2) * weights\n",
    "        prios = loss + 0.01\n",
    "        loss  = loss.mean()\n",
    "        loss.backward()\n",
    "        self.Q_network.optimizer.step()\n",
    "        self.priority_replay_buffer.update_priorities(indices, prios.data.cpu().numpy())\n",
    "        \n",
    "    def save(self, path):\n",
    "        torch.save(self.Q_network.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.Q_network.load_state_dict(torch.load(path))\n",
    "    def update_beta(self,betas):\n",
    "            self.priority_replay_buffer.betas_increment(betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, env, env_name, episodes, decrement, save_path, target_update_freq, batch_size,truncated):\n",
    "    # Lists to store scores, epsilons, and x-axis values\n",
    "    x_axis, scores, epsilons = [], [], []\n",
    "    betas=0.4\n",
    "    for i in range(episodes):\n",
    "        score = 0\n",
    "        done = False\n",
    "        state = env.reset()[0]\n",
    "        steps = 0\n",
    "        betas += 0.6 / (episodes) \n",
    "        if betas >=1:\n",
    "            betas=1\n",
    "\n",
    "        while not done:\n",
    "            # Choose an action based on the current state\n",
    "            action = agent.choose_action(state)\n",
    "            agent.update_beta(betas)\n",
    "            # Take a step in the environment\n",
    "            state_, reward, done,_, info = env.step(action)\n",
    "            agent.priority_replay_buffer.add(state, action, reward, state_, done)  # Add the experience to the replay buffer\n",
    "\n",
    "            if len(agent.priority_replay_buffer) > batch_size:\n",
    "                states, actions, rewards, states_, dones, indices, weights = agent.priority_replay_buffer.sample(batch_size)  # Sample a batch of experiences\n",
    "                agent.learn(states, actions, rewards, states_, dones, indices, weights)  # Train the agent with the sampled exper\n",
    "\n",
    "            state = state_\n",
    "            score += reward\n",
    "            steps += 1\n",
    "\n",
    "            # Terminate episode early for CartPole and Acrobot environments\n",
    "            if env_name == 'CartPole-v1':\n",
    "                if steps > 2000:\n",
    "                    break\n",
    "            elif env_name == 'Acrobot-v1':\n",
    "                if steps > 300:\n",
    "                    break\n",
    "            elif env_name ==\"LunarLander-v2\":\n",
    "                if steps > 1000:\n",
    "                    break\n",
    "            if steps > truncated:\n",
    "                print(truncated)\n",
    "                print(steps)\n",
    "                break\n",
    "        # Decrement epsilon for exploration-exploitation trade-off\n",
    "        agent.decrement_epsilon(decrement)\n",
    "\n",
    "        # Update the target network periodically\n",
    "        if i % target_update_freq == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        # Store scores and epsilons at specified intervals\n",
    "        x_axis.append(i + 1)\n",
    "        scores.append(score)\n",
    "        epsilons.append(agent.epsilon)\n",
    "\n",
    "        # Print episode information\n",
    "        if (i + 1) % (save_score_freq + 4) == 0:\n",
    "            avg_score = np.mean(scores[-save_score_freq+4:]) if len(scores) > 0 else 0.0\n",
    "            print(f'Episode {i + 1}, last score {score:.1f}, avg score {avg_score:.1f}, epsilon {agent.epsilon:.2f}')\n",
    "\n",
    "    # Save the trained agent\n",
    "    agent.save(save_path)\n",
    "\n",
    "    return x_axis, scores, epsilons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_environment_information(env_name,state_dim,action_dim,arc):\n",
    "        print(f'------------------------')\n",
    "        print(f'Current environment: {env_name}')\n",
    "        print(f'state dim {state_dim}')\n",
    "        print(f'action dim {action_dim}')\n",
    "        print(f'------------------------')\n",
    "        print(f'Current architecture: {arc}')\n",
    "        print(f'------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(all_scores,save_path,x_axis,epsilons,episodes,arc):\n",
    "    fig, ax = plt.subplots()\n",
    "        # Calculate and plot the average score curve\n",
    "    mean_scores = np.mean(all_scores, axis=0)\n",
    "    np.savetxt(f'{save_path}_mean', mean_scores, delimiter=\",\")\n",
    "    ax.plot(x_axis, mean_scores, 'k-', label='Average Score')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('score', color='b')\n",
    "\n",
    "    # Plot the epsilon curve on the same plot using a second y-axis.\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(x_axis, epsilons, 'r-', label='epsilon')\n",
    "    ax2.set_ylabel('epsilon', color='r')\n",
    "\n",
    "    # Combine the legends from both y-axes.\n",
    "    lines1, labels1 = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax.legend(lines1 + lines2, labels1 + labels2, loc='lower left')\n",
    "\n",
    "    # Set the title of the plot\n",
    "    plot_title = f\"Episodes: {episodes} - {arc} Q Network\"\n",
    "    plt.title(plot_title)\n",
    "\n",
    "    # Save the plot and display it\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment and truncation\n",
    "environments = ['CartPole-v1','Acrobot-v1','LunarLander-v2']\n",
    "n_episodes = [500]\n",
    "truncation = {'CartPole-v1': 2000, 'Acrobot-v1': 300, 'LunarLander-v2': 800}\n",
    "\n",
    "# Define testing Architectures and exploration percentage\n",
    "architecture = [\"Single\",\"Double\",\"Double Dueling\",\"Dueling\"]\n",
    "validation_amount = 5\n",
    "exploration_percentage = 0.70 \n",
    "\n",
    "# Set other parameters for training\n",
    "target_update_freq = 5\n",
    "buffer_size = 500\n",
    "batch_size = 64\n",
    "learning_rate = 0.0005\n",
    "gamma = 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Current environment: CartPole-v1\n",
      "state dim (4,)\n",
      "action dim 2\n",
      "------------------------\n",
      "Current architecture: Single\n",
      "------------------------\n",
      "Number of episodes in this run: 500\n",
      "------------------------\n",
      "Episode 5, last score 14.0, avg score 16.5, epsilon 0.99\n",
      "Episode 10, last score 21.0, avg score 18.6, epsilon 0.97\n",
      "Episode 15, last score 42.0, avg score 19.8, epsilon 0.96\n",
      "Episode 20, last score 19.0, avg score 20.7, epsilon 0.94\n",
      "Episode 25, last score 14.0, avg score 22.2, epsilon 0.93\n",
      "Episode 30, last score 18.0, avg score 21.1, epsilon 0.92\n",
      "Episode 35, last score 16.0, avg score 21.1, epsilon 0.90\n",
      "Episode 40, last score 20.0, avg score 20.3, epsilon 0.89\n",
      "Episode 45, last score 13.0, avg score 20.5, epsilon 0.87\n",
      "Episode 50, last score 18.0, avg score 20.7, epsilon 0.86\n",
      "Episode 55, last score 14.0, avg score 20.4, epsilon 0.84\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19636\\1692226731.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[1;31m# Train the agent and obtain scores and epsilons\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 x_axis, scores, epsilons = train_agent(agent, env, env_name, episodes, decrement, save_path,\\\n\u001b[1;32m---> 41\u001b[1;33m                                                         target_update_freq, batch_size,truncated)\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[1;31m# Store scores for this validation run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19636\\3883080195.py\u001b[0m in \u001b[0;36mtrain_agent\u001b[1;34m(agent, env, env_name, episodes, decrement, save_path, target_update_freq, batch_size, truncated)\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpriority_replay_buffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpriority_replay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Sample a batch of experiences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Train the agent with the sampled exper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19636\\4219498264.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, states, actions, rewards, states_, dones, indices, weights)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[1;31m# Use the online network to predict the Q-values for the current states and select actions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[0mq_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;31m# Use the target network to predict the Q-values for the next states.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19636\\4219498264.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mlayer1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mlayer2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mlayer3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m             \u001b[0mqvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\joaoP\\anaconda3\\envs\\Sistemas\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\joaoP\\anaconda3\\envs\\Sistemas\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Iterate over each environment\n",
    "for env_name in environments:\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # Get the dimension of the state and action spaces\n",
    "    state_dim = env.reset()[0].shape\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # Create an array to store average scores for each architecture and validation run\n",
    "    average_scores = np.zeros((len(architecture), validation_amount))\n",
    "    truncated = truncation[env_name]\n",
    "\n",
    "    # Iterate over each architecture\n",
    "    for arc_idx, arc in enumerate(architecture):\n",
    "        print_environment_information(env_name,state_dim,action_dim,arc)\n",
    "\n",
    "        # Create a list to store scores for each validation run\n",
    "        all_scores = [[] for _ in range(validation_amount)]\n",
    "        \n",
    "        # Iterate over the number of episodes\n",
    "        for episodes in n_episodes:\n",
    "            i=0\n",
    "            # Run the specified number of validation runs\n",
    "            for _ in range(validation_amount):\n",
    "                print(f'Number of episodes in this run: {episodes}')\n",
    "                print(f'------------------------')\n",
    "\n",
    "                # Set exploration and decrement values\n",
    "                decrement = 0.99 / (episodes * exploration_percentage)\n",
    "\n",
    "                # Set save path for model\n",
    "                save_path = f'models_priority{exploration_percentage}/{env_name}/{arc}/{env_name}_{episodes}_episodes'\n",
    "\n",
    "                # Create a replay buffer and agent\n",
    "                agent = Agent(input_dims=state_dim, n_actions=action_dim,arc=arc,buffer_size=buffer_size \\\n",
    "                              ,gamma=gamma,lr=learning_rate)\n",
    "\n",
    "                # Train the agent and obtain scores and epsilons\n",
    "                x_axis, scores, epsilons = train_agent(agent, env, env_name, episodes, decrement, save_path,\\\n",
    "                                                        target_update_freq, batch_size,truncated)\n",
    "\n",
    "                # Store scores for this validation run\n",
    "                all_scores[i] = scores\n",
    "                i += 1\n",
    "            \n",
    "            # Calculate and plot the average score curve\n",
    "            plot_results(all_scores,save_path,x_axis,epsilons,episodes,arc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "models_priority2/Acrobot-v1/Single/Acrobot-v1_100_episodes_mean not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19636\\3744548344.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0marcs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marchitectures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mdata_paths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbase_data_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marcs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mmean_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_mean_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[0mepsilon_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_epsilon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexploration_percentage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0mplot_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_plot_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19636\\3744548344.py\u001b[0m in \u001b[0;36mread_mean_scores\u001b[1;34m(file_paths)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmean_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mmean_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmean_scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\joaoP\\anaconda3\\envs\\Sistemas\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[0;32m   1065\u001b[0m             \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1067\u001b[1;33m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1068\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'encoding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\joaoP\\anaconda3\\envs\\Sistemas\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\joaoP\\anaconda3\\envs\\Sistemas\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    531\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[0;32m    532\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s not found.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: models_priority2/Acrobot-v1/Single/Acrobot-v1_100_episodes_mean not found."
     ]
    }
   ],
   "source": [
    "def read_mean_scores(file_paths):\n",
    "    mean_scores = []\n",
    "    for file_path in file_paths:\n",
    "        scores = np.loadtxt(file_path)\n",
    "        mean_scores.append(scores)\n",
    "    return mean_scores\n",
    "\n",
    "def calculate_epsilon(episodes, exploration_percentage):\n",
    "    epsilon = 1.0\n",
    "    decrement = 0.99 / (episodes * exploration_percentage)\n",
    "    epsilon_values = [epsilon]\n",
    "    for _ in range(episodes - 1):\n",
    "        epsilon -= decrement\n",
    "        if epsilon < 0:\n",
    "            epsilon = 0\n",
    "        epsilon_values.append(epsilon)\n",
    "    return epsilon_values\n",
    "\n",
    "def moving_average(x, y, window_size, use_moving_average=True):\n",
    "    if use_moving_average:\n",
    "        weights = np.repeat(1.0, window_size) / window_size\n",
    "        smoothed = np.convolve(y, weights, 'valid')\n",
    "        x_smoothed = x[window_size // 2: -(window_size // 2) + 1]\n",
    "        return x_smoothed, smoothed\n",
    "    else:\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def plot_numbers(episodes, env, architectures, num_episodes, epsilon_values, save_path):\n",
    "    fig, ax = plt.subplots()\n",
    "    x_axis = np.arange(1, num_episodes + 1)\n",
    "\n",
    "    for i, arc in enumerate(architectures):\n",
    "        numbers = episodes[i]\n",
    "        x_smoothed, numbers_smoothed = moving_average(x_axis, numbers, 12)\n",
    "        ax.plot(x_smoothed, numbers_smoothed, label=f\"{arc}\")\n",
    "\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('score', color='b')\n",
    "    ax.axhline(y=-100, color='black', linestyle='--')  # Add a black horizontal line at y=200\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(x_axis, epsilon_values, 'r-', label='epsilon')\n",
    "    ax2.set_ylabel('epsilon', color='r')\n",
    "\n",
    "    lines1, labels1 = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    all_lines = lines1 + lines2\n",
    "    all_labels = labels1 + labels2\n",
    "\n",
    "    legend = ax.legend(all_lines, all_labels, loc='lower right')  # Change legend position to 'lower right'\n",
    "    legend.set_bbox_to_anchor((0.335, -0.02))  # Adjust the legend position by changing the values (1, 0.9)\n",
    "    legend.get_frame().set_linewidth(0.5)  # Reduce the legend frame thickness\n",
    "    legend.fontsize='small'\n",
    "\n",
    "    plot_title = f\"{env} Episodes: {num_episodes}\"\n",
    "    ax.set_title(plot_title)\n",
    "\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "base_data_path = \"models_priority2/{env}/{arc}/{env}_{episodes}_episodes_mean\"\n",
    "base_plot_path = \"models_priority2/{env}/{env}_{episodes}_plotted_mean\"\n",
    "episode_numbers = [100, 250,500]\n",
    "environments = ['Acrobot-v1'] \n",
    "architectures = [[\"Single\",\"Double\",\"Double Dueling\",\"Dueling\"]]\n",
    "exploration_percentage = 0.40\n",
    "\n",
    "for env in environments:\n",
    "    for num_episodes in episode_numbers:\n",
    "        for arcs in architectures:\n",
    "            data_paths = [base_data_path.format(env=env, arc=arc, episodes=num_episodes) for arc in arcs]\n",
    "            mean_scores = read_mean_scores(data_paths)\n",
    "            epsilon_values = calculate_epsilon(num_episodes, exploration_percentage)\n",
    "            plot_path = base_plot_path.format(env=env, episodes=num_episodes)\n",
    "            plot_numbers(mean_scores, env, arcs, num_episodes, epsilon_values, plot_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sistemas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
